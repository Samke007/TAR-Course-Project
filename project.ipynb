{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAR Project\n",
    "\n",
    "## Topic: Stress Analysis in Social Media\n",
    "\n",
    "Social media users are active more than ever, posting new content on daily. On the other\n",
    "hand, stress has become extremely prominent and easily observable at the same time.\n",
    "Given the data collected on Reddit, your task is to build a system that would recognize\n",
    "posts riddled with stress markers, i.e., states of mental or emotional tension. Considering\n",
    "the nature of Reddit as a platform, the statements span across multiple domains, which\n",
    "imposes the necessity of broad natural language understanding. Essentially, the system\n",
    "boils down to classifying whether a post displays aspects of stress or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping torch as it is not installed.\n",
      "WARNING: Skipping transformers as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.0.1-cp38-cp38-win_amd64.whl (172.4 MB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting jinja2\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from torch) (4.6.2)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.24.3-cp38-cp38-win_amd64.whl (14.9 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.5.5-cp38-cp38-win_amd64.whl (267 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Using cached huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from transformers) (23.1)\n",
      "Collecting pyyaml>=5.1\n",
      "  Using cached PyYAML-6.0-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Collecting fsspec\n",
      "  Using cached fsspec-2023.5.0-py3-none-any.whl (160 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-2.1.2-cp38-cp38-win_amd64.whl (16 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp38-cp38-win_amd64.whl (96 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.2-py3-none-any.whl (123 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, tqdm, requests, pyyaml, mpmath, MarkupSafe, fsspec, filelock, tokenizers, sympy, regex, numpy, networkx, jinja2, huggingface-hub, transformers, torch\n",
      "Successfully installed MarkupSafe-2.1.2 certifi-2023.5.7 charset-normalizer-3.1.0 filelock-3.12.0 fsspec-2023.5.0 huggingface-hub-0.14.1 idna-3.4 jinja2-3.1.2 mpmath-1.3.0 networkx-3.1 numpy-1.24.3 pyyaml-6.0 regex-2023.5.5 requests-2.31.0 sympy-1.12 tokenizers-0.13.3 torch-2.0.1 tqdm-4.65.0 transformers-4.29.2 urllib3-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jakov\\Desktop\\Diplomski_studij\\2. semestar\\Analiza_i_pretraživanje_teksta\\APT_Projekt\\tar-project\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall torch transformers\n",
    "%pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.1-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Collecting keras\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from nltk) (2023.5.5)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Using cached tensorflow_intel-2.12.0-cp38-cp38-win_amd64.whl (272.8 MB)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting numpy>=1.20.3\n",
      "  Using cached numpy-1.23.5-cp38-cp38-win_amd64.whl (14.7 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.6.2)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.54.2-cp38-cp38-win_amd64.whl (4.1 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.10-py3-none-any.whl\n",
      "Requirement already satisfied: packaging in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (56.0.0)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp38-cp38-win_amd64.whl (35 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.23.2-cp38-cp38-win_amd64.whl (422 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.8.0-cp38-cp38-win_amd64.whl (2.7 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-win_amd64.whl (1.5 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting scipy>=1.7\n",
      "  Using cached scipy-1.10.1-cp38-cp38-win_amd64.whl (42.2 MB)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Using cached ml_dtypes-0.1.0-cp38-cp38-win_amd64.whl (120 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.31.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.19.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting urllib3<2.0\n",
      "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (6.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.15.0)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using legacy 'setup.py install' for sklearn, since package 'wheel' is not installed.\n",
      "Installing collected packages: urllib3, pyasn1, rsa, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, numpy, google-auth, wheel, werkzeug, tensorboard-data-server, scipy, protobuf, opt-einsum, ml-dtypes, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, libclang, keras, jax, h5py, google-pasta, gast, flatbuffers, astunparse, tzdata, tensorflow-intel, pytz, joblib, click, tensorflow, sklearn, pandas, nltk\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.2\n",
      "    Uninstalling urllib3-2.0.2:\n",
      "      Successfully uninstalled urllib3-2.0.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.3\n",
      "    Uninstalling numpy-1.24.3:\n",
      "      Successfully uninstalled numpy-1.24.3\n",
      "    Running setup.py install for sklearn: started\n",
      "    Running setup.py install for sklearn: finished with status 'done'\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 click-8.1.3 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.19.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.2 h5py-3.8.0 jax-0.4.10 joblib-1.2.0 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 nltk-3.8.1 numpy-1.23.5 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-2.0.1 protobuf-4.23.2 pyasn1-0.5.0 pyasn1-modules-0.3.0 pytz-2023.3 requests-oauthlib-1.3.1 rsa-4.9 scipy-1.10.1 sklearn-0.0.post5 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-intel-2.12.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 tzdata-2023.3 urllib3-1.26.16 werkzeug-2.3.4 wheel-0.40.0 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jakov\\Desktop\\Diplomski_studij\\2. semestar\\Analiza_i_pretraživanje_teksta\\APT_Projekt\\tar-project\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas nltk sklearn tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_id</th>\n",
       "      <th>sentence_range</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>confidence</th>\n",
       "      <th>social_timestamp</th>\n",
       "      <th>social_karma</th>\n",
       "      <th>syntax_ari</th>\n",
       "      <th>...</th>\n",
       "      <th>lex_dal_min_pleasantness</th>\n",
       "      <th>lex_dal_min_activation</th>\n",
       "      <th>lex_dal_min_imagery</th>\n",
       "      <th>lex_dal_avg_activation</th>\n",
       "      <th>lex_dal_avg_imagery</th>\n",
       "      <th>lex_dal_avg_pleasantness</th>\n",
       "      <th>social_upvote_ratio</th>\n",
       "      <th>social_num_comments</th>\n",
       "      <th>syntax_fk_grade</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>8601tu</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>He said he had not felt that way before, sugge...</td>\n",
       "      <td>33181</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1521614353</td>\n",
       "      <td>5</td>\n",
       "      <td>1.806818</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.77000</td>\n",
       "      <td>1.52211</td>\n",
       "      <td>1.89556</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1</td>\n",
       "      <td>3.253573</td>\n",
       "      <td>-0.002742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>assistance</td>\n",
       "      <td>8lbrx9</td>\n",
       "      <td>(0, 5)</td>\n",
       "      <td>Hey there r/assistance, Not sure if this is th...</td>\n",
       "      <td>2606</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1527009817</td>\n",
       "      <td>4</td>\n",
       "      <td>9.429737</td>\n",
       "      <td>...</td>\n",
       "      <td>1.125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.69586</td>\n",
       "      <td>1.62045</td>\n",
       "      <td>1.88919</td>\n",
       "      <td>0.65</td>\n",
       "      <td>2</td>\n",
       "      <td>8.828316</td>\n",
       "      <td>0.292857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptsd</td>\n",
       "      <td>9ch1zh</td>\n",
       "      <td>(15, 20)</td>\n",
       "      <td>My mom then hit me with the newspaper and it s...</td>\n",
       "      <td>38816</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1535935605</td>\n",
       "      <td>2</td>\n",
       "      <td>7.769821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.1429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.83088</td>\n",
       "      <td>1.58108</td>\n",
       "      <td>1.85828</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0</td>\n",
       "      <td>7.841667</td>\n",
       "      <td>0.011894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relationships</td>\n",
       "      <td>7rorpp</td>\n",
       "      <td>[5, 10]</td>\n",
       "      <td>until i met my new boyfriend, he is amazing, h...</td>\n",
       "      <td>239</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1516429555</td>\n",
       "      <td>0</td>\n",
       "      <td>2.667798</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.75356</td>\n",
       "      <td>1.52114</td>\n",
       "      <td>1.98848</td>\n",
       "      <td>0.50</td>\n",
       "      <td>5</td>\n",
       "      <td>4.104027</td>\n",
       "      <td>0.141671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>survivorsofabuse</td>\n",
       "      <td>9p2gbc</td>\n",
       "      <td>[0, 5]</td>\n",
       "      <td>October is Domestic Violence Awareness Month a...</td>\n",
       "      <td>1421</td>\n",
       "      <td>1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1539809005</td>\n",
       "      <td>24</td>\n",
       "      <td>7.554238</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.1250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.77644</td>\n",
       "      <td>1.64872</td>\n",
       "      <td>1.81456</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>7.910952</td>\n",
       "      <td>-0.204167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit post_id sentence_range   \n",
       "0              ptsd  8601tu       (15, 20)  \\\n",
       "1        assistance  8lbrx9         (0, 5)   \n",
       "2              ptsd  9ch1zh       (15, 20)   \n",
       "3     relationships  7rorpp        [5, 10]   \n",
       "4  survivorsofabuse  9p2gbc         [0, 5]   \n",
       "\n",
       "                                                text     id  label   \n",
       "0  He said he had not felt that way before, sugge...  33181      1  \\\n",
       "1  Hey there r/assistance, Not sure if this is th...   2606      0   \n",
       "2  My mom then hit me with the newspaper and it s...  38816      1   \n",
       "3  until i met my new boyfriend, he is amazing, h...    239      1   \n",
       "4  October is Domestic Violence Awareness Month a...   1421      1   \n",
       "\n",
       "   confidence  social_timestamp  social_karma  syntax_ari  ...   \n",
       "0         0.8        1521614353             5    1.806818  ...  \\\n",
       "1         1.0        1527009817             4    9.429737  ...   \n",
       "2         0.8        1535935605             2    7.769821  ...   \n",
       "3         0.6        1516429555             0    2.667798  ...   \n",
       "4         0.8        1539809005            24    7.554238  ...   \n",
       "\n",
       "   lex_dal_min_pleasantness  lex_dal_min_activation  lex_dal_min_imagery   \n",
       "0                     1.000                  1.1250                  1.0  \\\n",
       "1                     1.125                  1.0000                  1.0   \n",
       "2                     1.000                  1.1429                  1.0   \n",
       "3                     1.000                  1.1250                  1.0   \n",
       "4                     1.000                  1.1250                  1.0   \n",
       "\n",
       "   lex_dal_avg_activation  lex_dal_avg_imagery  lex_dal_avg_pleasantness   \n",
       "0                 1.77000              1.52211                   1.89556  \\\n",
       "1                 1.69586              1.62045                   1.88919   \n",
       "2                 1.83088              1.58108                   1.85828   \n",
       "3                 1.75356              1.52114                   1.98848   \n",
       "4                 1.77644              1.64872                   1.81456   \n",
       "\n",
       "   social_upvote_ratio  social_num_comments  syntax_fk_grade  sentiment  \n",
       "0                 0.86                    1         3.253573  -0.002742  \n",
       "1                 0.65                    2         8.828316   0.292857  \n",
       "2                 0.67                    0         7.841667   0.011894  \n",
       "3                 0.50                    5         4.104027   0.141671  \n",
       "4                 1.00                    1         7.910952  -0.204167  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"dreaddit-train.csv\")\n",
    "test_df = pd.read_csv(\"dreaddit-test.csv\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will check the sizes of our datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2838, 116)\n",
      "(715, 116)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'post_id', 'sentence_range', 'text', 'id', 'label',\n",
       "       'confidence', 'social_timestamp', 'social_karma', 'syntax_ari',\n",
       "       ...\n",
       "       'lex_dal_min_pleasantness', 'lex_dal_min_activation',\n",
       "       'lex_dal_min_imagery', 'lex_dal_avg_activation', 'lex_dal_avg_imagery',\n",
       "       'lex_dal_avg_pleasantness', 'social_upvote_ratio',\n",
       "       'social_num_comments', 'syntax_fk_grade', 'sentiment'],\n",
       "      dtype='object', length=116)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "train_df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we will choose three models and use them for classification of posts into two classes: positive or negative for stress. The three models that we chose are Naive Bayes Classifier, LSTM and BERT. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached scikit_learn-1.2.2-cp38-cp38-win_amd64.whl (8.3 MB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.2.2 threadpoolctl-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jakov\\Desktop\\Diplomski_studij\\2. semestar\\Analiza_i_pretraživanje_teksta\\APT_Projekt\\tar-project\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jakov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Jakov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Jakov\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7034965034965035\n",
      "Confusion matrix:\n",
      "[[194 152]\n",
      " [ 60 309]]\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.56      0.65       346\n",
      "           1       0.67      0.84      0.74       369\n",
      "\n",
      "    accuracy                           0.70       715\n",
      "   macro avg       0.72      0.70      0.70       715\n",
      "weighted avg       0.72      0.70      0.70       715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Preprocess the text data\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stop words and punctuation\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words and token.isalpha()]\n",
    "    # Lemmatize the tokens\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Join the tokens back into a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "train_vectors = vectorizer.fit_transform(train_df['text'])\n",
    "test_vectors = vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_vectors, train_df['label'])\n",
    "\n",
    "\n",
    "file = open(\"naive_bayes.txt\", \"w\")\n",
    "\n",
    "# Evaluate the classifier on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(test_df)):\n",
    "    text = test_df.loc[i, 'text']\n",
    "    true_label = test_df.loc[i, 'label']\n",
    "    vector = vectorizer.transform([text])\n",
    "    predicted_label = clf.predict(vector)[0]\n",
    "    print(f\"Text: {text}\", file=file)\n",
    "    print(f\"Predicted label: {predicted_label}\", file=file)\n",
    "    print(f\"True label: {true_label}\", file=file)\n",
    "    print(file=file)\n",
    "    if predicted_label == true_label:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "predicted_labels = clf.predict(test_vectors)\n",
    "cm = confusion_matrix(test_df[\"label\"], predicted_labels)\n",
    "print(\"Confusion matrix:\")\n",
    "print(cm)\n",
    "\n",
    "cl_report = classification_report(test_df[\"label\"], predicted_labels)\n",
    "print(\"Classification report:\")\n",
    "print(cl_report)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.12.0-cp38-cp38-win_amd64.whl (272.8 MB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.4.10)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (67.6.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.2)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.19.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.28.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.3.4)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.0.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: tensorflow-intel, tensorflow\n",
      "Successfully installed tensorflow-2.12.0 tensorflow-intel-2.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36/36 [==============================] - 12s 265ms/step - loss: 0.6940 - accuracy: 0.5696 - val_loss: 0.6387 - val_accuracy: 0.7201\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 10s 268ms/step - loss: 0.6145 - accuracy: 0.7938 - val_loss: 0.6214 - val_accuracy: 0.7271\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 10s 271ms/step - loss: 0.4885 - accuracy: 0.8665 - val_loss: 0.4941 - val_accuracy: 0.7553\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 10s 267ms/step - loss: 0.2929 - accuracy: 0.8903 - val_loss: 0.5054 - val_accuracy: 0.7553\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 10s 266ms/step - loss: 0.1509 - accuracy: 0.9581 - val_loss: 0.5766 - val_accuracy: 0.7465\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 10s 268ms/step - loss: 0.0773 - accuracy: 0.9802 - val_loss: 0.6538 - val_accuracy: 0.7165\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 10s 268ms/step - loss: 0.0337 - accuracy: 0.9934 - val_loss: 0.8018 - val_accuracy: 0.7236\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 10s 269ms/step - loss: 0.0170 - accuracy: 0.9969 - val_loss: 0.9219 - val_accuracy: 0.7324\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 10s 272ms/step - loss: 0.0122 - accuracy: 0.9982 - val_loss: 0.9356 - val_accuracy: 0.7042\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 10s 292ms/step - loss: 0.0101 - accuracy: 0.9982 - val_loss: 0.9905 - val_accuracy: 0.6937\n",
      "23/23 [==============================] - 2s 45ms/step\n",
      "Accuracy: 0.6559\n",
      "Confusion Matrix:\n",
      "[[225 121]\n",
      " [125 244]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.65      0.65       346\n",
      "           1       0.67      0.66      0.66       369\n",
      "\n",
      "    accuracy                           0.66       715\n",
      "   macro avg       0.66      0.66      0.66       715\n",
      "weighted avg       0.66      0.66      0.66       715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens if token.lower() not in stop_words and token.isalpha()]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
    "test_df['test'] = test_df['text'].apply(preprocess_text)\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Pad the sequences to have the same length\n",
    "max_sequence_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "# Create the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_sequence_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(train_sequences, train_df['label'], epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the LSTM model on the test set\n",
    "y_pred_probs = model.predict(test_sequences)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "y_true = test_df['label'].values\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy = (y_pred.flatten() == y_true).mean()\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "\n",
    "file = open(\"lstm.txt\", \"w\")\n",
    "# Print predictions and true labels for each record\n",
    "for i in range(len(test_df)):\n",
    "    text = test_df.iloc[i]['text']\n",
    "    predicted_label = y_pred[i][0]\n",
    "    true_label = y_true[i]\n",
    "    print(f'Text: {text}', file=file)\n",
    "    print(f'Predicted Label: {predicted_label}', file=file)\n",
    "    print(f'True Label: {true_label}', file=file)\n",
    "    print(file=file)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Compute the classification report\n",
    "report = classification_report(y_true, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### SVM with tf-idf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (0.1.99)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: protobuf==3.20.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (3.20.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install protobuf==3.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -entencepiece (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\jakov\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7216783216783217\n",
      "Confusion Matrix:\n",
      "[[233 113]\n",
      " [ 86 283]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70       346\n",
      "           1       0.71      0.77      0.74       369\n",
      "\n",
      "    accuracy                           0.72       715\n",
      "   macro avg       0.72      0.72      0.72       715\n",
      "weighted avg       0.72      0.72      0.72       715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "# Prepare the data\n",
    "X_train = train_df['text']\n",
    "y_train = train_df['label']\n",
    "X_test = test_df['text']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# Create the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training and test data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Create and train the SVM model\n",
    "model = SVC()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "confusion_mat = confusion_matrix(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Write results to a file\n",
    "with open(\"svm_results.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    for text, true_label, pred_label in zip(X_test, y_test, y_pred):\n",
    "        print(f'Text: {text}', file=file)\n",
    "        print(f'Predicted Label: {pred_label}', file=file)\n",
    "        print(f'True Label: {true_label}', file=file)\n",
    "        print(file=file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing a statistical difference of the performance difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The difference in accuracy is not statistically significant.\n",
      "The difference in accuracy is not statistically significant.\n",
      "The difference in accuracy is not statistically significant.\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Accuracy scores for the two models\n",
    "model1_metrics = [53, 0.54, 0.55, 0.55]\n",
    "model2_metrics = [70, 0.67, 0.84, 0.74]\n",
    "model3_metrics = [66, 0.67, 0.66, 0.66]\n",
    "model4_metrics = [72, 0.71, 0.77, 0.74]\n",
    "\n",
    "\n",
    "t_statistic23, p_value23 = stats.ttest_ind(model2_metrics, model3_metrics)\n",
    "t_statistic24, p_value24 = stats.ttest_ind(model2_metrics, model4_metrics)\n",
    "t_statistic34, p_value34 = stats.ttest_ind(model3_metrics, model4_metrics)\n",
    "\n",
    "\n",
    "\n",
    "# Check if the difference is statistically significant\n",
    "\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value23 < alpha:\n",
    "    print(\"The difference in accuracy is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in accuracy is not statistically significant.\")\n",
    "\n",
    "if p_value24 < alpha:\n",
    "    print(\"The difference in accuracy is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in accuracy is not statistically significant.\")\n",
    "\n",
    "if p_value34 < alpha:\n",
    "    print(\"The difference in accuracy is statistically significant.\")\n",
    "else:\n",
    "    print(\"The difference in accuracy is not statistically significant.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit_posthocsNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached scikit_posthocs-0.7.0-py3-none-any.whl (38 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "Requirement already satisfied: scipy in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from scikit_posthocs) (1.10.1)\n",
      "Requirement already satisfied: pandas>=0.20.0 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from scikit_posthocs) (2.0.1)\n",
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.0-cp38-cp38-win_amd64.whl (9.4 MB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.7.1-cp38-cp38-win_amd64.whl (7.6 MB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from scikit_posthocs) (1.23.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from pandas>=0.20.0->scikit_posthocs) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from pandas>=0.20.0->scikit_posthocs) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from pandas>=0.20.0->scikit_posthocs) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.20.0->scikit_posthocs) (1.16.0)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting importlib-resources>=3.2.0\n",
      "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp38-cp38-win_amd64.whl (55 kB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.0.7-cp38-cp38-win_amd64.whl (162 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-win_amd64.whl (2.5 MB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from matplotlib->scikit_posthocs) (23.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\jakov\\desktop\\diplomski_studij\\2. semestar\\analiza_i_pretraživanje_teksta\\apt_projekt\\tar-project\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->scikit_posthocs) (3.15.0)\n",
      "Collecting patsy>=0.5.2\n",
      "  Using cached patsy-0.5.3-py2.py3-none-any.whl (233 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, importlib-resources, fonttools, cycler, contourpy, patsy, matplotlib, statsmodels, seaborn, scikit-posthocs\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 importlib-resources-5.12.0 kiwisolver-1.4.4 matplotlib-3.7.1 patsy-0.5.3 pillow-9.5.0 pyparsing-3.0.9 scikit-posthocs-0.7.0 seaborn-0.12.2 statsmodels-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\Jakov\\Desktop\\Diplomski_studij\\2. semestar\\Analiza_i_pretraživanje_teksta\\APT_Projekt\\tar-project\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit_posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman Test:\n",
      "Friedman statistic: 6.310344827586208\n",
      "p-value: 0.09744972655019858\n",
      "\n",
      "Nemenyi Test:\n",
      "          1         2         3\n",
      "1  1.000000  0.412512  0.989128\n",
      "2  0.412512  1.000000  0.335143\n",
      "3  0.989128  0.335143  1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import friedmanchisquare\n",
    "from scikit_posthocs import posthoc_nemenyi\n",
    "\n",
    "# Define the performance matrix with rows representing models and columns representing evaluation metrics\n",
    "performance_matrix = np.array([\n",
    "    [70, 0.67, 0.84, 0.74],\n",
    "    [66, 0.67, 0.66, 0.66],\n",
    "    [72, 0.71, 0.77, 0.74]\n",
    "])\n",
    "\n",
    "# Perform the Friedman test\n",
    "friedman_stat, p_value = friedmanchisquare(*performance_matrix.T)\n",
    "print(\"Friedman Test:\")\n",
    "print(\"Friedman statistic:\", friedman_stat)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "# Perform the Nemenyi post-hoc test\n",
    "nemenyi_results = posthoc_nemenyi(performance_matrix)\n",
    "print(\"\\nNemenyi Test:\")\n",
    "print(nemenyi_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
